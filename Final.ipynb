{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module1():\n",
    "    try:\n",
    "        while(1):\n",
    "            r = sr.Recognizer()\n",
    "            mic = sr.Microphone()\n",
    "            print('Talk now')\n",
    "            with mic as source:\n",
    "                audio = r.listen(source)\n",
    "            with open(\"audio_file.wav\", \"wb\") as file:\n",
    "                file.write(audio.get_wav_data())\n",
    "            print(\"Processing..\")\n",
    "            file_audio = sr.AudioFile('audio_file.wav')\n",
    "            with file_audio as source:\n",
    "                audio_text = r.record(source)\n",
    "            text=r.recognize_google(audio_text)\n",
    "            string=text\n",
    "            char= {'a': (1,1), 'b' : (1,2), 'c' : (1,3), 'd' : (1,4), 'e' : (1,5),\n",
    "                    'f' : (2,1), 'g' : (2,2), 'h' : (2,3), 'i' : (2,4), 'j' : (2,5),\n",
    "                   'l' : (3,1), 'm' : (3,2), 'n' : (3,3), 'o' : (3,4), 'p' : (3,5),\n",
    "                   'q' : (4,1), 'r' : (4,2), 's' : (4,3), 't' : (4,4), 'u' : (4,5),\n",
    "                   'v' : (5,1), 'w' : (5,2), 'x' : (5,3), 'y' : (5,4), 'z' : (5,5), 'k' : (1,3),\n",
    "                   '0' : (1,0), '1' : (1,6), '2' : (1,7), '3' : (1,8), '4' : (1,9),\n",
    "                   '5' : (2,0), '6' : (2,6), '7' : (2,7), '8' : (2,8), '9' : (2,9),\n",
    "                   ' ' : (1,10), '\\'':(1,10), '.': (1,10)      }\n",
    "            string=string.lower()\n",
    "            print(string)\n",
    "            i = 0\n",
    "            while i < len(string):\n",
    "                temp = string[i]\n",
    "                count = char[string[i]][0]\n",
    "                setting = char[string[i]][1]\n",
    "                while(count > 0):  \n",
    "                    print(char[string[i]][1], end=\"\")\n",
    "                    count = count - 1\n",
    "                print(\".\",end=\"\")\n",
    "                i = i + 1\n",
    "                \n",
    "            print('\\n')\n",
    "    except Exception:\n",
    "        print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module2():\n",
    "    my_model = load_model('myhand9.h5')\n",
    "    path = 'roi.jpg'\n",
    "    # Capture video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Decrease frame size\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1000)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 600)\n",
    "    # Import font\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # Continuous detection\n",
    "    sent=\"\"\n",
    "    tletter=\"\"\n",
    "    lcount=0\n",
    "    while(1):\n",
    "    #   Capturing individual frames\n",
    "        ret, frame = cap.read()\n",
    "    #   Region of interest box setup\n",
    "        x,y,w,h=80,220,400,350\n",
    "        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        roi=frame[y:y+h, x:x+w]\n",
    "    #   Saving roi image to jpg file\n",
    "        cv2.imwrite('roi.jpg',roi)\n",
    "    #   Load and convert image to numpy array\n",
    "        img1=image.load_img(path, target_size=(64, 64))  \n",
    "        x=image.img_to_array(img1)\n",
    "        x=np.expand_dims(x, axis=0)\n",
    "        images = np.vstack([x])\n",
    "    #   Send image array to model to predict class\n",
    "        pred_class=my_model.predict_classes(images, batch_size=10)\n",
    "    #   Convert predicted class to letter\n",
    "        pred_class=str(pred_class)[1:-1]\n",
    "        pred_class=int(pred_class)\n",
    "        letter=chr(pred_class+65)\n",
    "        if (letter == \"\\\\\"):\n",
    "            letter=\"None\"\n",
    "        if letter == \"[\":\n",
    "            letter=\"Del\"\n",
    "        if letter == \"]\":\n",
    "            letter=\"Space\"\n",
    "    #   Display predicted letter\n",
    "        cv2.putText(frame,str(letter),(50,100),font,2,(0,0,255),2)\n",
    "    #   Make sentence\n",
    "        if(tletter==letter):\n",
    "            lcount=lcount+1\n",
    "        else:\n",
    "            lcount=0\n",
    "        if(lcount==25):\n",
    "            if(letter==\"None\"):\n",
    "                letter=\"\"\n",
    "            if(letter==\"Space\"):\n",
    "                letter=\" \"\n",
    "            if(letter==\"Del\"):\n",
    "                sent=sent[ : -1]\n",
    "            else:\n",
    "                sent=sent+letter\n",
    "            lcount=0\n",
    "        tletter=letter\n",
    "    #   Display predicted sentence\n",
    "        cv2.putText(frame,str(sent),(300,100),font,2,(0,0,255),2)\n",
    "        cv2.imshow('ASL_Prediction',frame)    \n",
    "    #   Close the output video by pressing 'ESC'\n",
    "        k = cv2.waitKey(5) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    mytext =sent\n",
    "    print(mytext)\n",
    "    print(\"\\n\")\n",
    "    # Language in which you want to convert \n",
    "    language = 'en'\n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "    myobj.save(\"speechop.wav\") \n",
    "    os.system(\"speechop.wav\")\n",
    "    # Release camera \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter module to be used:\n",
      "1 for Module 1\n",
      "2 for Module 2\n",
      "3 to terminate\n",
      "\n",
      "2\n",
      "\n",
      "Module 2 selected\n",
      "\n",
      "KRITHIKA\n",
      "\n",
      "\n",
      "Enter module to be used:\n",
      "1 for Module 1\n",
      "2 for Module 2\n",
      "3 to terminate\n",
      "\n",
      "2\n",
      "\n",
      "Module 2 selected\n",
      "\n",
      "SHYAMA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    module=int(input(\"Enter module to be used:\\n1 for Module 1\\n2 for Module 2\\n3 to terminate\\n\\n\"))\n",
    "    # module=1\n",
    "    if(module==1):\n",
    "        print(\"\\nModule 1 selected\\n\")\n",
    "        module1()\n",
    "    elif(module==2):\n",
    "        print(\"\\nModule 2 selected\\n\")\n",
    "        module2()\n",
    "    elif(module==3):\n",
    "        print(\"\\nProgram Terminated\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"\\nInvalid Module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
